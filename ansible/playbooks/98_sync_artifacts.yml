---
- name: Sync de artefactos (DAGs Airflow, configs Kafka, jobs Spark)
  hosts: "{{ targets | default(sync_target_group | default('all')) }}"
  become: true

  vars:
    # Raíz local en el controlador donde viven los artefactos
    iac_root: "/home/ansible/IaC"
    # Preferir rsync (si no está, cae a copy)
    prefer_rsync: true

    # Control granular de borrado (evita staleness). Por defecto SOLO Airflow DAGs.
    rsync_delete_dags: true
    rsync_delete_kafka: false
    rsync_delete_spark: false

  pre_tasks:
    - name: Mostrar grupo/hosts destino
      ansible.builtin.debug:
        msg: "Destino: {{ ansible_play_hosts_all }}"

    - name: Normalizar prefer_rsync en variable efectiva
      ansible.builtin.set_fact:
        prefer_rsync_effective: "{{ (prefer_rsync | default(true)) | bool }}"

    - name: Asegurar rsync si prefer_rsync=true (solo Debian/Ubuntu)
      ansible.builtin.package:
        name: rsync
        state: present
      when: prefer_rsync_effective
      ignore_errors: true

    # ====== STAT de rutas locales (en el controlador) ======
    - name: Stat Airflow DAGs (local)
      ansible.builtin.stat:
        path: "{{ sync_map.airflow.src_dags | default('') }}"
      register: st_airflow_dags
      delegate_to: localhost
      run_once: true
      when: sync_map.airflow is defined and (sync_map.airflow.src_dags | default('')) != ''

    - name: Stat Airflow plugins (local)
      ansible.builtin.stat:
        path: "{{ sync_map.airflow.src_plugins | default('') }}"
      register: st_airflow_plugins
      delegate_to: localhost
      run_once: true
      when: sync_map.airflow is defined and (sync_map.airflow.src_plugins | default('')) != ''

    - name: Stat Kafka config (local)
      ansible.builtin.stat:
        path: "{{ sync_map.kafka.src_config | default('') }}"
      register: st_kafka_config
      delegate_to: localhost
      run_once: true
      when: sync_map.kafka is defined and (sync_map.kafka.src_config | default('')) != ''

    - name: Stat Kafka scripts (local)
      ansible.builtin.stat:
        path: "{{ sync_map.kafka.src_scripts | default('') }}"
      register: st_kafka_scripts
      delegate_to: localhost
      run_once: true
      when: sync_map.kafka is defined and (sync_map.kafka.src_scripts | default('')) != ''

    - name: Stat Spark jobs (local)
      ansible.builtin.stat:
        path: "{{ sync_map.spark.src_jobs | default('') }}"
      register: st_spark_jobs
      delegate_to: localhost
      run_once: true
      when: sync_map.spark is defined and (sync_map.spark.src_jobs | default('')) != ''

    - name: Stat Spark conf (local)
      ansible.builtin.stat:
        path: "{{ sync_map.spark.src_conf | default('') }}"
      register: st_spark_conf
      delegate_to: localhost
      run_once: true
      when: sync_map.spark is defined and (sync_map.spark.src_conf | default('')) != ''

    # ====== Crear árboles destino ======
    - name: Build ensure_dirs
      ansible.builtin.set_fact:
        ensure_dirs: >-
          {{
            [
              (sync_map.airflow.dest_dags     | default(None)),
              (sync_map.airflow.dest_plugins  | default(None)),
              (sync_map.kafka.dest_config     | default(None)),
              (sync_map.kafka.dest_scripts    | default(None)),
              (sync_map.spark.dest_jobs       | default(None)),
              (sync_map.spark.dest_conf       | default(None))
            ] | select('defined') | select('string')
          }}

    # --- Validación NO FATAL de UIDs/GIDs (solo warnings) ---
    - name: Validate users (UID) (non-fatal)
      ansible.builtin.command: "getent passwd {{ item.uid }}"
      register: _uid
      changed_when: false
      failed_when: false
      loop:
        - { name: "airflow", uid: "{{ sync_map.airflow.uid | default(omit) }}" }
        - { name: "kafka",   uid: "{{ sync_map.kafka.uid   | default(omit) }}" }
        - { name: "spark",   uid: "{{ sync_map.spark.uid   | default(omit) }}" }

    - name: Warn if UID missing
      ansible.builtin.debug:
        msg: "WARNING: UID {{ item.item.uid }} ({{ item.item.name }}) no existe en {{ inventory_hostname }}; se hará chown por UID numérico."
      when: item.rc is defined and item.rc != 0
      loop: "{{ _uid.results | default([]) }}"

    - name: Validate groups (GID) (non-fatal)
      ansible.builtin.command: "getent group {{ item.gid }}"
      register: _gid
      changed_when: false
      failed_when: false
      loop:
        - { name: "airflow", gid: "{{ sync_map.airflow.gid | default(omit) }}" }
        - { name: "kafka",   gid: "{{ sync_map.kafka.gid   | default(omit) }}" }
        - { name: "spark",   gid: "{{ sync_map.spark.gid   | default(omit) }}" }

    - name: Warn if GID missing
      ansible.builtin.debug:
        msg: "WARNING: GID {{ item.item.gid }} ({{ item.item.name }}) no existe en {{ inventory_hostname }}; se hará chown por GID numérico."
      when: item.rc is defined and item.rc != 0
      loop: "{{ _gid.results | default([]) }}"

    - name: Crear árboles destino
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        mode: "0775"
      loop: "{{ ensure_dirs | default([]) }}"

    - name: Permisos (Airflow)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        recurse: true
        owner: "{{ sync_map.airflow.uid | default(50000) }}"
        group: "{{ sync_map.airflow.gid | default(50000) }}"
      loop:
        - "{{ sync_map.airflow.dest_dags | default(omit) }}"
        - "{{ sync_map.airflow.dest_plugins | default(omit) }}"
      when: sync_map.airflow is defined

    - name: Permisos (Kafka)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        recurse: true
        owner: "{{ sync_map.kafka.uid | default(1000) }}"
        group: "{{ sync_map.kafka.gid | default(1000) }}"
      loop:
        - "{{ sync_map.kafka.dest_config | default(omit) }}"
        - "{{ sync_map.kafka.dest_scripts | default(omit) }}"
      when: sync_map.kafka is defined

    - name: Permisos (Spark)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        recurse: true
        owner: "{{ sync_map.spark.uid | default(1000) }}"
        group: "{{ sync_map.spark.gid | default(1000) }}"
      loop:
        - "{{ sync_map.spark.dest_jobs | default(omit) }}"
        - "{{ sync_map.spark.dest_conf | default(omit) }}"
      when: sync_map.spark is defined

  tasks:
    # ====== AIRFLOW ======
    - name: Sync Airflow DAGs (rsync)
      ansible.posix.synchronize:
        src: "{{ sync_map.airflow.src_dags }}/"
        dest: "{{ sync_map.airflow.dest_dags }}/"
        delete: "{{ rsync_delete_dags | default(true) }}"
        recursive: true
      delegate_to: localhost
      when:
        - sync_map.airflow is defined
        - prefer_rsync_effective
        - st_airflow_dags is defined and st_airflow_dags.stat.exists

    - name: Sync Airflow DAGs (fallback copy)
      ansible.builtin.copy:
        src: "{{ sync_map.airflow.src_dags }}/"
        dest: "{{ sync_map.airflow.dest_dags }}/"
        mode: "u=rwX,g=rX,o=rX"
      when:
        - sync_map.airflow is defined
        - (not prefer_rsync_effective)
        - st_airflow_dags is defined and st_airflow_dags.stat.exists

    - name: Sync Airflow plugins (rsync)
      ansible.posix.synchronize:
        src: "{{ sync_map.airflow.src_plugins }}/"
        dest: "{{ sync_map.airflow.dest_plugins }}/"
        delete: false
        recursive: true
      delegate_to: localhost
      when:
        - sync_map.airflow is defined
        - prefer_rsync_effective
        - st_airflow_plugins is defined and st_airflow_plugins.stat.exists

    - name: Sync Airflow plugins (fallback copy)
      ansible.builtin.copy:
        src: "{{ sync_map.airflow.src_plugins }}/"
        dest: "{{ sync_map.airflow.dest_plugins }}/"
        mode: "u=rwX,g=rX,o=rX"
      when:
        - sync_map.airflow is defined
        - (not prefer_rsync_effective)
        - st_airflow_plugins is defined and st_airflow_plugins.stat.exists

    # ====== KAFKA ======
    - name: Sync Kafka config (rsync)
      ansible.posix.synchronize:
        src: "{{ sync_map.kafka.src_config }}/"
        dest: "{{ sync_map.kafka.dest_config }}/"
        delete: "{{ rsync_delete_kafka | default(false) }}"
        recursive: true
      delegate_to: localhost
      when:
        - sync_map.kafka is defined
        - prefer_rsync_effective
        - st_kafka_config is defined and st_kafka_config.stat.exists

    - name: Sync Kafka config (fallback copy)
      ansible.builtin.copy:
        src: "{{ sync_map.kafka.src_config }}/"
        dest: "{{ sync_map.kafka.dest_config }}/"
        mode: "u=rwX,g=rX,o=rX"
      when:
        - sync_map.kafka is defined
        - (not prefer_rsync_effective)
        - st_kafka_config is defined and st_kafka_config.stat.exists

    - name: Sync Kafka scripts (rsync)
      ansible.posix.synchronize:
        src: "{{ sync_map.kafka.src_scripts }}/"
        dest: "{{ sync_map.kafka.dest_scripts }}/"
        delete: "{{ rsync_delete_kafka | default(false) }}"
        recursive: true
      delegate_to: localhost
      when:
        - sync_map.kafka is defined
        - prefer_rsync_effective
        - st_kafka_scripts is defined and st_kafka_scripts.stat.exists

    - name: Sync Kafka scripts (fallback copy - ejecutables)
      ansible.builtin.copy:
        src: "{{ sync_map.kafka.src_scripts }}/"
        dest: "{{ sync_map.kafka.dest_scripts }}/"
        mode: "0755"
      when:
        - sync_map.kafka is defined
        - (not prefer_rsync_effective)
        - st_kafka_scripts is defined and st_kafka_scripts.stat.exists

    # ====== SPARK ======
    - name: Sync Spark jobs (rsync)
      ansible.posix.synchronize:
        src: "{{ sync_map.spark.src_jobs }}/"
        dest: "{{ sync_map.spark.dest_jobs }}/"
        delete: "{{ rsync_delete_spark | default(false) }}"
        recursive: true
      delegate_to: localhost
      when:
        - sync_map.spark is defined
        - prefer_rsync_effective
        - st_spark_jobs is defined and st_spark_jobs.stat.exists

    - name: Sync Spark jobs (fallback copy)
      ansible.builtin.copy:
        src: "{{ sync_map.spark.src_jobs }}/"
        dest: "{{ sync_map.spark.dest_jobs }}/"
        mode: "u=rwX,g=rX,o=rX"
      when:
        - sync_map.spark is defined
        - (not prefer_rsync_effective)
        - st_spark_jobs is defined and st_spark_jobs.stat.exists

    - name: Sync Spark conf (rsync)
      ansible.posix.synchronize:
        src: "{{ sync_map.spark.src_conf }}/"
        dest: "{{ sync_map.spark.dest_conf }}/"
        delete: "{{ rsync_delete_spark | default(false) }}"
        recursive: true
      delegate_to: localhost
      when:
        - sync_map.spark is defined
        - prefer_rsync_effective
        - st_spark_conf is defined and st_spark_conf.stat.exists

    - name: Sync Spark conf (fallback copy)
      ansible.builtin.copy:
        src: "{{ sync_map.spark.src_conf }}/"
        dest: "{{ sync_map.spark.dest_conf }}/"
        mode: "u=rwX,g=rX,o=rX"
      when:
        - sync_map.spark is defined
        - (not prefer_rsync_effective)
        - st_spark_conf is defined and st_spark_conf.stat.exists

  post_tasks:
    - name: Listado final de destinos
      ansible.builtin.shell: |
        set -e
        echo "AIRFLOW DAGS:";    ls -la "{{ sync_map.airflow.dest_dags    | default('/dev/null') }}" || true
        echo "AIRFLOW plugins:"; ls -la "{{ sync_map.airflow.dest_plugins | default('/dev/null') }}" || true
        echo "KAFKA config:";    ls -la "{{ sync_map.kafka.dest_config     | default('/dev/null') }}" || true
        echo "KAFKA scripts:";   ls -la "{{ sync_map.kafka.dest_scripts    | default('/dev/null') }}" || true
        echo "SPARK jobs:";      ls -la "{{ sync_map.spark.dest_jobs       | default('/dev/null') }}" || true
        echo "SPARK conf:";      ls -la "{{ sync_map.spark.dest_conf       | default('/dev/null') }}" || true
      args:
        executable: /bin/bash
      register: listout
      changed_when: false

    - ansible.builtin.debug:
        var: listout.stdout_lines


# ============================
#   PLAY 2: MON / PROMETHEUS
# ============================
- name: Sync de artefactos (configs Prometheus, DAGs, etc.)
  hosts: all
  gather_facts: false
  become: true
  vars:
    prefer_rsync: true
  tasks:
    - name: Normalizar prefer_rsync en variable efectiva
      ansible.builtin.set_fact:
        prefer_rsync_effective: "{{ (prefer_rsync | default(true)) | bool }}"

    - name: Crear destino conf Prometheus (si existe sync_map.mon)
      when:
        - hostvars[inventory_hostname].inventory_hostname == "mon-core"
        - sync_map.mon is defined
      ansible.builtin.file:
        path: "{{ sync_map.mon.dest_prom_conf }}"
        state: directory
        owner: "{{ sync_map.mon.uid | default(0) }}"
        group: "{{ sync_map.mon.gid | default(0) }}"
        mode: "0755"

    - name: Sincronizar conf Prometheus (rsync si disponible)
      when:
        - hostvars[inventory_hostname].inventory_hostname == "mon-core"
        - sync_map.mon is defined
      block:
        - name: Con rsync
          ansible.posix.synchronize:
            src: "{{ sync_map.mon.src_prom_conf }}/"
            dest: "{{ sync_map.mon.dest_prom_conf }}/"
            rsync_opts:
              - "--delete"
          delegate_to: localhost
          when: prefer_rsync_effective

        - name: Sin rsync (fallback)
          ansible.builtin.copy:
            src: "{{ sync_map.mon.src_prom_conf }}/"
            dest: "{{ sync_map.mon.dest_prom_conf }}/"
            owner: "{{ sync_map.mon.uid | default(0) }}"
            group: "{{ sync_map.mon.gid | default(0) }}"
            mode: "0644"
          when: not prefer_rsync_effective