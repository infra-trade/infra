---
- name: Sync de artefactos (DAGs Airflow, configs Kafka, jobs Spark)
  hosts: "{{ targets | default(sync_target_group | default('all')) }}"
  become: true

  vars:
    # Puedes sobreescribirlos desde group_vars
    iac_root: "{{ iac_root | default('/home/ansible/IaC') }}"
    prefer_rsync: "{{ prefer_rsync | default(true) }}"

  pre_tasks:
    - name: Mostrar grupo/hosts destino
      ansible.builtin.debug:
        msg: "Destino: {{ ansible_play_hosts_all }}"

    - name: Asegurar rsync si prefer_rsync=true (solo Debian/Ubuntu)
      ansible.builtin.package:
        name: rsync
        state: present
      when: prefer_rsync | bool
      ignore_errors: true

    # ====== STAT de rutas locales (en el controlador) ======
    - name: Stat Airflow DAGs (local)
      ansible.builtin.stat:
        path: "{{ sync_map.airflow.src_dags | default('') }}"
      register: st_airflow_dags
      delegate_to: localhost
      run_once: true
      when: sync_map.airflow is defined and (sync_map.airflow.src_dags | default('')) != ''

    - name: Stat Airflow plugins (local)
      ansible.builtin.stat:
        path: "{{ sync_map.airflow.src_plugins | default('') }}"
      register: st_airflow_plugins
      delegate_to: localhost
      run_once: true
      when: sync_map.airflow is defined and (sync_map.airflow.src_plugins | default('')) != ''

    - name: Stat Kafka config (local)
      ansible.builtin.stat:
        path: "{{ sync_map.kafka.src_config | default('') }}"
      register: st_kafka_config
      delegate_to: localhost
      run_once: true
      when: sync_map.kafka is defined and (sync_map.kafka.src_config | default('')) != ''

    - name: Stat Kafka scripts (local)
      ansible.builtin.stat:
        path: "{{ sync_map.kafka.src_scripts | default('') }}"
      register: st_kafka_scripts
      delegate_to: localhost
      run_once: true
      when: sync_map.kafka is defined and (sync_map.kafka.src_scripts | default('')) != ''

    - name: Stat Spark jobs (local)
      ansible.builtin.stat:
        path: "{{ sync_map.spark.src_jobs | default('') }}"
      register: st_spark_jobs
      delegate_to: localhost
      run_once: true
      when: sync_map.spark is defined and (sync_map.spark.src_jobs | default('')) != ''

    - name: Stat Spark conf (local)
      ansible.builtin.stat:
        path: "{{ sync_map.spark.src_conf | default('') }}"
      register: st_spark_conf
      delegate_to: localhost
      run_once: true
      when: sync_map.spark is defined and (sync_map.spark.src_conf | default('')) != ''

    # ====== Crear árboles destino ======
    - name: Build ensure_dirs
      ansible.builtin.set_fact:
        ensure_dirs: >-
          {{
            [
              (sync_map.airflow.dest_dags     | default(None)),
              (sync_map.airflow.dest_plugins  | default(None)),
              (sync_map.kafka.dest_config     | default(None)),
              (sync_map.kafka.dest_scripts    | default(None)),
              (sync_map.spark.dest_jobs       | default(None)),
              (sync_map.spark.dest_conf       | default(None))
            ] | select('defined') | select('string')
          }}

    - name: Crear árboles destino
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        mode: "0775"
      loop: "{{ ensure_dirs | default([]) }}"

    # ====== Validar usuarios/grupos (evitar root-owned inesperado) ======
    - name: Validate users/groups exist for stacks (UID)
      ansible.builtin.command: "getent passwd {{ item.uid }}"
      register: _uid
      failed_when: _uid.rc != 0
      changed_when: false
      when: item.uid is defined
      loop:
        - { name: "airflow", uid: "{{ sync_map.airflow.uid | default(omit) }}" }
        - { name: "kafka",   uid: "{{ sync_map.kafka.uid   | default(omit) }}" }
        - { name: "spark",   uid: "{{ sync_map.spark.uid   | default(omit) }}" }

    - name: Validate users/groups exist for stacks (GID)
      ansible.builtin.command: "getent group {{ item.gid }}"
      register: _gid
      failed_when: _gid.rc != 0
      changed_when: false
      when: item.gid is defined
      loop:
        - { name: "airflow", gid: "{{ sync_map.airflow.gid | default(omit) }}" }
        - { name: "kafka",   gid: "{{ sync_map.kafka.gid   | default(omit) }}" }
        - { name: "spark",   gid: "{{ sync_map.spark.gid   | default(omit) }}" }

    # ====== Permisos base por stack ======
    - name: Permisos (Airflow)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        recurse: true
        owner: "{{ sync_map.airflow.uid | default(50000) }}"
        group: "{{ sync_map.airflow.gid | default(50000) }}"
      loop:
        - "{{ sync_map.airflow.dest_dags | default(omit) }}"
        - "{{ sync_map.airflow.dest_plugins | default(omit) }}"
      when: sync_map.airflow is defined

    - name: Permisos (Kafka)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        recurse: true
        owner: "{{ sync_map.kafka.uid | default(1000) }}"
        group: "{{ sync_map.kafka.gid | default(1000) }}"
      loop:
        - "{{ sync_map.kafka.dest_config | default(omit) }}"
        - "{{ sync_map.kafka.dest_scripts | default(omit) }}"
      when: sync_map.kafka is defined

    - name: Permisos (Spark)
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        recurse: true
        owner: "{{ sync_map.spark.uid | default(1000) }}"
        group: "{{ sync_map.spark.gid | default(1000) }}"
      loop:
        - "{{ sync_map.spark.dest_jobs | default(omit) }}"
        - "{{ sync_map.spark.dest_conf | default(omit) }}"
      when: sync_map.spark is defined

    # ====== Prechecks (lint/parsing/version) ======
    - block:
        - name: Python lint (ruff) para DAGs
          ansible.builtin.command: "{{ precheck.python_bin }} -m ruff check {{ sync_map.airflow.src_dags }}"
          delegate_to: localhost
          register: ruff_result
          changed_when: false
          when: sync_map.airflow is defined and st_airflow_dags.stat.exists

        - name: Airflow DAGs parsing
          ansible.builtin.command: "{{ precheck.airflow_cmd }} dags list --report"
          delegate_to: localhost
          register: airflow_list
          changed_when: false
          when: sync_map.airflow is defined and st_airflow_dags.stat.exists

        - name: Spark sanity (version)
          ansible.builtin.command: "{{ precheck.spark_submit_cmd }} --version"
          delegate_to: localhost
          register: spark_v
          changed_when: false
          when: sync_map.spark is defined and (st_spark_jobs.stat.exists or st_spark_conf.stat.exists)
      when: precheck.enable | default(false)
      rescue:
        - name: Abort on precheck failure
          ansible.builtin.fail:
            msg: "Precheck failed. ruff={{ ruff_result.rc | default('n/a') }}, airflow={{ airflow_list.rc | default('n/a') }}, spark={{ spark_v.rc | default('n/a') }}"

  tasks:
    # ====== AIRFLOW (Atomic deploy de DAGs) ======
    - name: Prepare Airflow releases/current (vars)
      ansible.builtin.set_fact:
        release_ts: "{{ ansible_date_time.iso8601_basic }}"
        airflow_releases: "{{ (sync_map.airflow.dest_dags | dirname) + '/releases' }}"
        airflow_current:  "{{ (sync_map.airflow.dest_dags | dirname) + '/current' }}"
      when: (atomic_deploy.enable | default(false)) and (sync_map.airflow is defined) and (st_airflow_dags.stat.exists | default(false))

    - name: Create releases dir
      ansible.builtin.file:
        path: "{{ airflow_releases }}"
        state: directory
        mode: "0775"
      when: (atomic_deploy.enable | default(false)) and (sync_map.airflow is defined) and (st_airflow_dags.stat.exists | default(false))

    - name: Rsync DAGs to new release
      ansible.posix.synchronize:
        src: "{{ sync_map.airflow.src_dags }}/"
        dest: "{{ airflow_releases }}/{{ release_ts }}/"
        recursive: true
        delete: "{{ delete_policy.airflow_dags | default(false) }}"
        rsync_opts: >-
          {{
            [
              "--chmod=Du=rwx,Dg=rx,Fu=rw,Fg=r",
              (airflow_dags_exclude_file is defined) | ternary("--exclude-from=" ~ airflow_dags_exclude_file, omit)
            ] | reject('equalto', omit)
          }}
      delegate_to: localhost
      when: (atomic_deploy.enable | default(false)) and (sync_map.airflow is defined) and (st_airflow_dags.stat.exists | default(false)) and (prefer_rsync | bool)

    - name: Point current -> release (atomic symlink swap)
      ansible.builtin.file:
        src: "{{ airflow_releases }}/{{ release_ts }}"
        dest: "{{ airflow_current }}"
        state: link
        force: true
      when: (atomic_deploy.enable | default(false)) and (sync_map.airflow is defined) and (st_airflow_dags.stat.exists | default(false))

    - name: Ensure scheduler reads from symlinked path
      ansible.builtin.file:
        path: "{{ sync_map.airflow.dest_dags }}"
        state: link
        src: "{{ airflow_current }}"
        force: true
      when: (atomic_deploy.enable | default(false)) and (sync_map.airflow is defined) and (st_airflow_dags.stat.exists | default(false))

    - name: Prune old releases
      ansible.builtin.shell: |
        ls -1dt "{{ airflow_releases }}"/* | tail -n +{{ (atomic_deploy.keep_releases | default(5) | int) + 1 }} | xargs -r rm -rf
      args:
        executable: /bin/bash
      when: (atomic_deploy.enable | default(false)) and (sync_map.airflow is defined) and (st_airflow_dags.stat.exists | default(false))

    # Fallback: Sync DAGs sin atomic (o cuando prefer_rsync=false)
    - name: Sync Airflow DAGs (rsync simple)
      ansible.posix.synchronize:
        src: "{{ sync_map.airflow.src_dags }}/"
        dest: "{{ sync_map.airflow.dest_dags }}/"
        delete: "{{ delete_policy.airflow_dags | default(false) }}"
        recursive: true
        rsync_opts: >-
          {{
            [
              "--chmod=Du=rwx,Dg=rx,Fu=rw,Fg=r",
              (airflow_dags_exclude_file is defined) | ternary("--exclude-from=" ~ airflow_dags_exclude_file, omit)
            ] | reject('equalto', omit)
          }}
      delegate_to: localhost
      when:
        - sync_map.airflow is defined
        - st_airflow_dags is defined and st_airflow_dags.stat.exists
        - (not (atomic_deploy.enable | default(false))) or (not prefer_rsync | bool)

    - name: Sync Airflow DAGs (fallback copy)
      ansible.builtin.copy:
        src: "{{ sync_map.airflow.src_dags }}/"
        dest: "{{ sync_map.airflow.dest_dags }}/"
        mode: "0644"
      when:
        - sync_map.airflow is defined
        - st_airflow_dags is defined and st_airflow_dags.stat.exists
        - (not prefer_rsync | bool)

    - name: Sync Airflow plugins (rsync)
      ansible.posix.synchronize:
        src: "{{ sync_map.airflow.src_plugins }}/"
        dest: "{{ sync_map.airflow.dest_plugins }}/"
        delete: "{{ delete_policy.airflow_plugins | default(false) }}"
        recursive: true
        rsync_opts:
          - "--chmod=Du=rwx,Dg=rx,Fu=rw,Fg=r"
      delegate_to: localhost
      when:
        - sync_map.airflow is defined
        - prefer_rsync | bool
        - st_airflow_plugins is defined and st_airflow_plugins.stat.exists

    - name: Sync Airflow plugins (fallback copy)
      ansible.builtin.copy:
        src: "{{ sync_map.airflow.src_plugins }}/"
        dest: "{{ sync_map.airflow.dest_plugins }}/"
        mode: "0644"
      when:
        - sync_map.airflow is defined
        - (not prefer_rsync | bool)
        - st_airflow_plugins is defined and st_airflow_plugins.stat.exists

    - name: Make plugin binaries executable (opcional)
      ansible.builtin.shell: |
        find "{{ sync_map.airflow.dest_plugins }}" -type f \( -name "*.sh" -o -name "*.py" -o -name "*.bin" \) -exec chmod 0755 {} +
      args:
        executable: /bin/bash
      when: sync_map.airflow is defined

    # ====== KAFKA ======
    - name: Sync Kafka config (rsync)
      ansible.posix.synchronize:
        src: "{{ sync_map.kafka.src_config }}/"
        dest: "{{ sync_map.kafka.dest_config }}/"
        recursive: true
        delete: "{{ delete_policy.kafka_config | default(false) }}"
        rsync_opts: >-
          {{
            [
              (kafka_conf_exclude_file is defined) | ternary("--exclude-from=" ~ kafka_conf_exclude_file, omit)
            ] | reject('equalto', omit)
          }}
      delegate_to: localhost
      when:
        - sync_map.kafka is defined
        - prefer_rsync | bool
        - st_kafka_config is defined and st_kafka_config.stat.exists

    - name: Sync Kafka config (fallback copy)
      ansible.builtin.copy:
        src: "{{ sync_map.kafka.src_config }}/"
        dest: "{{ sync_map.kafka.dest_config }}/"
        mode: "0644"
      when:
        - sync_map.kafka is defined
        - (not prefer_rsync | bool)
        - st_kafka_config is defined and st_kafka_config.stat.exists

    - name: Sync Kafka scripts (rsync)
      ansible.posix.synchronize:
        src: "{{ sync_map.kafka.src_scripts }}/"
        dest: "{{ sync_map.kafka.dest_scripts }}/"
        recursive: true
        delete: "{{ delete_policy.kafka_scripts | default(false) }}"
        rsync_opts:
          - "--chmod=Du=rwx,Dg=rx,Fu=rw,Fg=r"
      delegate_to: localhost
      when:
        - sync_map.kafka is defined
        - prefer_rsync | bool
        - st_kafka_scripts is defined and st_kafka_scripts.stat.exists

    - name: Sync Kafka scripts (fallback copy)
      ansible.builtin.copy:
        src: "{{ sync_map.kafka.src_scripts }}/"
        dest: "{{ sync_map.kafka.dest_scripts }}/"
        mode: "0755"
      when:
        - sync_map.kafka is defined
        - (not prefer_rsync | bool)
        - st_kafka_scripts is defined and st_kafka_scripts.stat.exists

    - name: Ensure executables under kafka/scripts
      ansible.builtin.file:
        path: "{{ item.path }}"
        mode: "0755"
      loop: "{{ query('ansible.builtin.find', paths=sync_map.kafka.dest_scripts, patterns='*', recurse=True) }}"
      when: sync_map.kafka is defined

    # ====== SPARK ======
    - name: Sync Spark jobs (rsync)
      ansible.posix.synchronize:
        src: "{{ sync_map.spark.src_jobs }}/"
        dest: "{{ sync_map.spark.dest_jobs }}/"
        recursive: true
        delete: "{{ delete_policy.spark_jobs | default(false) }}"
        rsync_opts:
          - "--chmod=Du=rwx,Dg=rx,Fu=rw,Fg=r"
      delegate_to: localhost
      when:
        - sync_map.spark is defined
        - prefer_rsync | bool
        - st_spark_jobs is defined and st_spark_jobs.stat.exists

    - name: Sync Spark jobs (fallback copy)
      ansible.builtin.copy:
        src: "{{ sync_map.spark.src_jobs }}/"
        dest: "{{ sync_map.spark.dest_jobs }}/ }}"
        mode: "0644"
      when:
        - sync_map.spark is defined
        - (not prefer_rsync | bool)
        - st_spark_jobs is defined and st_spark_jobs.stat.exists

    - name: Sync Spark conf (rsync)
      ansible.posix.synchronize:
        src: "{{ sync_map.spark.src_conf }}/"
        dest: "{{ sync_map.spark.dest_conf }}/"
        recursive: true
        delete: "{{ delete_policy.spark_conf | default(false) }}"
      delegate_to: localhost
      when:
        - sync_map.spark is defined
        - prefer_rsync | bool
        - st_spark_conf is defined and st_spark_conf.stat.exists

    - name: Sync Spark conf (fallback copy)
      ansible.builtin.copy:
        src: "{{ sync_map.spark.src_conf }}/"
        dest: "{{ sync_map.spark.dest_conf }}/"
        mode: "0644"
      when:
        - sync_map.spark is defined
        - (not prefer_rsync | bool)
        - st_spark_conf is defined and st_spark_conf.stat.exists

  post_tasks:
    - name: Listado final de destinos (seguro)
      vars:
        list_paths:
          - { label: "AIRFLOW DAGS",    path: "{{ sync_map.airflow.dest_dags    | default('/dev/null') }}" }
          - { label: "AIRFLOW plugins", path: "{{ sync_map.airflow.dest_plugins | default('/dev/null') }}" }
          - { label: "KAFKA config",    path: "{{ sync_map.kafka.dest_config    | default('/dev/null') }}" }
          - { label: "KAFKA scripts",   path: "{{ sync_map.kafka.dest_scripts   | default('/dev/null') }}" }
          - { label: "SPARK jobs",      path: "{{ sync_map.spark.dest_jobs      | default('/dev/null') }}" }
          - { label: "SPARK conf",      path: "{{ sync_map.spark.dest_conf      | default('/dev/null') }}" }
      ansible.builtin.shell: |
        set -e
        echo "{{ item.label }}:"
        ls -la "{{ item.path }}" || true
      args:
        executable: /bin/bash
      loop: "{{ list_paths }}"
      register: listouts
      changed_when: false

    - name: Mostrar listados
      ansible.builtin.debug:
        var: listouts.results | map(attribute='stdout_lines') | list

- name: Sync de artefactos (configs Prometheus, DAGs, etc.)
  hosts: all
  gather_facts: false
  become: true
  vars:
    prefer_rsync: true
  tasks:
    - name: Crear destino conf Prometheus (si existe sync_map.mon)
      when: hostvars[inventory_hostname].inventory_hostname == "mon-core" and (sync_map.mon is defined)
      ansible.builtin.file:
        path: "{{ sync_map.mon.dest_prom_conf }}"
        state: directory
        owner: "{{ sync_map.mon.uid | default(0) }}"
        group: "{{ sync_map.mon.gid | default(0) }}"
        mode: "0755"

    - name: Sincronizar conf Prometheus (rsync si disponible)
      when: hostvars[inventory_hostname].inventory_hostname == "mon-core" and (sync_map.mon is defined)
      block:
        - name: Con rsync
          ansible.posix.synchronize:
            src: "{{ sync_map.mon.src_prom_conf }}/"
            dest: "{{ sync_map.mon.dest_prom_conf }}/"
            rsync_opts:
              - "--delete"
          delegate_to: localhost
          when: prefer_rsync | bool

        - name: Sin rsync (fallback)
          ansible.builtin.copy:
            src: "{{ sync_map.mon.src_prom_conf }}/"
            dest: "{{ sync_map.mon.dest_prom_conf }}/"
            owner: "{{ sync_map.mon.uid | default(0) }}"
            group: "{{ sync_map.mon.gid | default(0) }}"
            mode: "0644"
          when: not prefer_rsync | bool