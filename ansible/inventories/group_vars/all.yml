portainer_url: "https://10.20.0.12:9443"
portainer_token: ""   # opcional si usas PORTAINER_TOKEN en el entorno
portainer_validate_certs: false

# Mapea tus endpoints reales
endpoints:
  mon: 2
  airflow: 3
  kafka: 4
  spark: 5
  db: 6

# Ruta a tu clave si usas SSH para repo privado (opcional)
github_deploy_key: "/home/ansible/.ssh/id_ed25519"

# Stacks a gestionar
stacks:
  - name: mon
    endpoint: mon
    repo_url: "git@github.com:infra-trade/infra.git"   # <-- mejor SSH para privado
    compose_path: "mon/compose/docker-compose.yml"
    git_ref: "main"
    env: []
  - name: airflow
    endpoint: airflow
    repo_url: "git@github.com:infra-trade/infra.git"
    compose_path: "airflow/compose/docker-compose.yml"
    git_ref: "main"
    env: []
  - name: kafka
    endpoint: kafka
    repo_url: "git@github.com:infra-trade/infra.git"
    compose_path: "kafka/compose/docker-compose.yml"
    git_ref: "main"
    env: []
  - name: spark
    endpoint: spark
    repo_url: "git@github.com:infra-trade/infra.git"
    compose_path: "spark/compose/docker-compose.yml"
    git_ref: "main"
    env: []
  - name: db
    endpoint: db
    repo_url: "git@github.com:infra-trade/infra.git"
    compose_path: "db/compose/docker-compose.yml"
    git_ref: "main"
    env: []


# Si no tienes algunas carpetas en el repo, no pasa nada: se omiten.
# Grupo/hosts destino (puedes sobreescribir con -e targets=host1,host2)
sync_target_group: "all"

# Ruta local del repo IaC en el controlador
iac_root: "/home/ansible/IaC"

# Dónde queremos los archivos en cada host Docker
sync_map:
  airflow:
    src_dags:     "{{ iac_root }}/airflow/dags"
    src_plugins:  "{{ iac_root }}/airflow/plugins"
    dest_dags:    "/srv/iac/airflow/dags"
    dest_plugins: "/srv/iac/airflow/plugins"
    # UID/GID típico de Airflow oficial (ajusta si tu imagen usa otros)
    uid: 50000
    gid: 50000

  kafka:
    src_config:  "{{ iac_root }}/kafka/config"
    src_scripts: "{{ iac_root }}/kafka/scripts"
    dest_config: "/srv/iac/kafka/config"
    dest_scripts: "/srv/iac/kafka/scripts"
    uid: 1000
    gid: 1000

  spark:
    src_jobs: "{{ iac_root }}/spark/jobs"
    src_conf: "{{ iac_root }}/spark/conf"
    dest_jobs: "/srv/iac/spark/jobs"
    dest_conf: "/srv/iac/spark/conf"
    uid: 1000
    gid: 1000

# Usa synchronize (rsync). Si no hay rsync en el remoto, cae a 'copy'.
prefer_rsync: true